Introduction to Big Data
1.Job Scope
Data Warehouse-原始数据的存储，要求retrieve的返回
Data Analysis-基于原始数据的分析，得出结论
Data Transformation-建模，分析，转化，结构化数据存储
Data Collection-信息收集，综合呈现，过滤
2.Hadoop in Industries
Google:Youtube Search Ranking--Personalized Search Ranking
Linkedin:People You May Know--Calculate distance between people
Netflix:Movie Recommendation--
Facebook:Store and Process log files

Introduction to Hadoop Ecosystem
1.What is hadoop?
Apache Hadoop is an open source software framework for storage and large scale processing of data-sets on clusters of commodity hardware.
2.Why is hadoop?
集群
分而治之：并行运行，速度和效率会大大提升；独立处理性能要求降低，控制成本
3.Hardoop Ecosystem
Data Engineer 主要掌握：HBase,HDFS,MapReduce,Hive(数据仓库工具)
-Now the data is coming in, what is the first thing to do?
--Storage on multiple machines
  HDFS:
  Data Storage
○ Data Split
○ Data replication
All you care about is the path of file: /hdfs/input/doc1.txt
  MapReduce:
  Data Processing
○ How to leverage job
○ How do nodes communicate
○ How to deal with node failure
  All you care about is input and output
  HBase:
  HBase is an open source, non-relational, distributed database
  Key-Value store
What if the input is a big file = 1T?Can we process it with one machine?
No!
--Out of memory
--Too slow
What should we do?
Use multiple machines
Issues may be caused by multiple machines：
  -Data Skew 数据倾斜
  -Node Failure
Then MapReduce Comes Out
MapReduce:
A programming model for processing and generating large data sets with a parallel, distributed algorithm on a cluster.

How to Solve Data Skew Problem:
数据倾斜是一个在MapReduce中可能会出现的问题，共有以下几种解决方案
1、增加jvm内存,这适用于第一种情况(唯一值非常少，极少数值有非常多的记录值(唯一值少于几千)),这种情况下,往往只能通过硬件的手段来进行调优,增加jvm内存可以显著的提高运行效率
2、增加reduce的个数,这适用于第二种情况(唯一值比较多，这个字段的某些值有远远多于其他值的记录数，但是它的占比也小于百分之一或千分之一),我们知道,这种情况下,最容易造成的结果就是大量相同key被partition到一个分区,从而一个reduce执行了大量的工作,而如果我们增加了reduce的个数,这种情况相对来说会减轻很多,毕竟计算的节点多了,就算工作量还是不均匀的,那也要小很多。
3、自定义分区,这需要用户自己继承partition类,指定分区策略,这种方式效果比较显著。
4、重新设计key,有一种方案是在map阶段时给key加上一个随机数,有了随机数的key就不会被大量的分配到同一节点(小几率),待到reduce后再把随机数去掉即可。
5、使用combinner合并,combinner是在map阶段,reduce之前的一个中间阶段,在这个阶段可以选择性的把大量的相同key数据先进行一个合并,可以看做是local reduce,然后再交给reduce来处理,这样做的好处很多,即减轻了map端向reduce端发送的数据量(减轻了网络带宽),也减轻了map端和reduce端中间的shuffle阶段的数据拉取数量(本地化磁盘IO速率),推荐使用这种方法。
workflow inside MapReduce:
1.Mapper:
  A stage
  ○ Splits into small words
  Implementation
  ○ https://hadoop.apache.org/docs/r2.6.2/api/org/apache/hadoop/mapreduce/Mapper.html
  ○ Simplify your work
2.Reducer: 
  A stage
  ○ Combines data from Mapper
  Implementation
  ○ https://hadoop.apache.org/docs/r2.6.2/api/org/apache/hadoop/mapreduce/Reducer.html
  ○ Simplify your work
 Hadoop的优点：
 1.Throughput
  ○ Run computation in parallel
 2.Scalability
  ○ Store and distribute very large data sets across clusters of hundreds of
  inexpensive servers operating in parallel
  3.Reliability
  ○ Even if individual nodes experience high rates of failure when running jobs on a
  large cluster, data is replicated across a cluster so that it can be recovered easily
  in the face of disk, node or rack failures
  4.Cheap
  ○ Cluster of inexpensive servers


Prepare for the Big Data Program:
● Algorithm
● System design
● Big data projects
● Implementation/Algorithm behind the projects
● Master one or two tools related to big data
